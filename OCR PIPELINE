OCR PIPELINE

Starts with a large directory of pdf files.
Assumes two things:
- You have the 4.0 release of Tesseract and it is properly trained on English text
	* Installation and training instructions here: https://github.com/tesseract-ocr/tesseract/wiki/4.0-with-LSTM
	* This should work with other versions of tesseract, but it's not recommended for very large sets of documents.
- Docsplit! Can be substituted for good old fashioned "convert" utility in imagemagick, which is what docsplit is using regardless.

Initial note: (To move a large number of files that exceed the bash buffer, do this)
	find ./ -name '*.png' -exec mv {}  pngs/ \;
Take care to keep directory sizes down and process requests in batches. With 100Ks of files in a directory simple things begin to act... badly.

I'm also playing around with trying to implement this in a way that works: https://gist.github.com/gtfierro/8324883

1. Convert to individual pages of PNGs. This will not name them in 
  > for file in *.pdf; do docsplit images "$file" --density 200 --format png; done
  To speed up this process, generate a list of commands to run sequentially and run in parallel

	Here's a how-to: https://stackoverflow.com/questions/5230436/how-to-quickly-generate-png-thumbnails-of-all-pages-of-a-pdf
	In this case, it's generated by executing the following:
	>#for f in *.pdf ; do echo "docsplit images "$f" --density 200 --format png" >> /tmp/runme ; done
	>edit:
	>	for f in *.pdf ; do echo "docsplit images "$f" --density 200 --format png && mv $f ../split_pdfs/" >> bash_scripts.txt ; done
	>wc -l bash_scripts.txt
		Spits out 21366
		Divided by 6 (3561) and used this to split
		Divide by 4 (better practice i assume) ceil 5342
	>split -l 5342 bash_scripts.txt
		This created 4 files in your current directory 
		xaa xab xac xad
	`chmod u+x` each of them
		then run 
	> sh ./xaa & ./xab & ./xac & ./xad & ./xae & ./xaf
		and it will generate 6 processes running at once. 
		note... if you stop this process, the first n-1 commands will run until they're done. If you have to, ps -A to find the processes and `kill [pid]` them
		[1] 2428
		[2] 2429
		[3] 2430
			It WILL give you the name of the processes running the background, so you got that going for you if that's the case.

	To unbreak things... This could be improved by moving the source file once it has been split into multiple pages to a processed directory. For very large tasks of breaking up these pages that would be very helpful

2. Note: docsplit doesn't z-fill the names of the sequential files it creates. We will fix this later with a python script (rename_and_combine.py). But before we go further, make sure your files do not have whitespace characters in them, as that will break the process to multithread. This works for me:
	> for f in *\ *; do mv "$f" "${f// /_}"; done 

3. Make your ocr.sh file. This will be called on each file. Its contents should look like this:
		#!/bin/bash
		export OMP_THREAD_LIMIT=1
		tesseract --oem 1 $1.png $1 pdf
	This makes it so that it will run with one thread at a time. This is important because we are multithreading for multiple OCR tasks running concurrently,
	Now run this to make the file executable
	> chmod u+x ocr.sh


4. you are now ready to ocr all the things. Run this in the directory:
	>time find . -name "*.png" -print0 | tr -d .png | tr -d ./ | xargs -0 -n 1 -P $(sysctl -n hw.ncpu) ../ocr.sh


	I'll try to break this out as well as I can.
	> time find . -name "*.png" -print0
		generate a list with all characters preserved (copes with weird filenames) and ending with a null character
	> tr -d .png
		... which pipes to this to erase the string ".png"
	> tr -d ./ 
		... which pipes to this to get rid of the leading "./" kicked out by -print0
	> xargs -0 -n 1 -P $(sysctl -n hw.ncpu) ../ocr.sh
		xargs:  a nasty utility to build long commands
		-0: inputs are separated by a null character (see -print0 in first pipe)
		-n 1: ONE argument per commandline, aka the file name
		-P: number of processes to run in parallel
		$(sysctl -n hw.ncpu): the number of cores on your computer
			so -P 4 for my computer  ....  :(
		../ocr.sh: send those inputs to ocr.sh, which runs an ocr task on one argument, the name of the file.

	Meanwhile.... I'm testing out this, which if it works will fix every damn things: 
		 /// > find . -name '*.pdf' -print0 | parallel --null -j20 pdf_ocr.sh {} ../scanned/{} \;
		> find . -name '*.png' -print0 | tr -d .png |  tr -d ./ | parallel --bar --null -j20 ../ocr2.sh {} ../parallel/{} \;
			This works with the rename/mv thing

		// find . -name '*.png' -print0 | tr -d .png | parallel --null -j20 ../ocr2.sh {} ../parallel/{} \; 
			-- i don't think that one works...
			

4. when all have finished running, move all pdfs into a base pdf directory 
	(in the top directory for this project)
	mkdir ocrd_pdfs
	mkdir processed_pdfs
	(in each directory)
	find ./ -name '*.pdf' -exec mv {}  ../processed_pdfs \;

5. Run rename_combine.py
	 This will move the items into the processed_pdfs/
	 Check the results and flush the ocrd_pdfs directory. You no longer need it. You can load in new files and run rename_combine as more results come in.

6. You have a lot of ocrd pdfs! Congratulations!